{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b59dc9",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a849c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d499865c",
   "metadata": {},
   "source": [
    "## Load a Simulated Dataset\n",
    "\n",
    "Similar to how we explored clustering, we will begin exploring Support Vector Machines (SVMs) by looking at how they preform on the simulated data sets below. There are a few advantages of starting with simulated data rather than a real world problem. First off, it allows us to generate data with specific properties, such as being difficult to separate, and secondly, by creating a 2D data set we ensure that we are able to view the entire data set in the plan and see what the SVM separation boundary looks like. After exploring these data sets, we will apply our knowledge of SVMs and kernels to a real data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data into a pandas data frame\n",
    "df = pd.read_csv('data/X1.csv')\n",
    "# Extract the values into a numpy array\n",
    "X1 = df.values\n",
    "# Create a scatter plot based on the data and the data labels\n",
    "plt.scatter(X1[:,0],X1[:,1],c=X1[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12fe988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('data/X2.csv')\n",
    "# Extract data frame into an array\n",
    "X2 = df.values\n",
    "# Make a scatter plot\n",
    "plt.scatter(X2[:,0],X2[:,1],c=X2[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d3f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import feature vectors\n",
    "df = pd.read_csv('data/X.csv')\n",
    "X3 = df.values\n",
    "# Import labels\n",
    "df = pd.read_csv('data/Y.csv')\n",
    "Y3 = df.values\n",
    "Y3 = Y3.flatten()\n",
    "# Create a scatter plot\n",
    "plt.scatter(X3[:, 0], X3[:, 1], c=Y3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b5a07d",
   "metadata": {},
   "source": [
    "Since we have loaded in 3 data sets above, the below cell lets us set which data set we want to work for the time being. It starts being set to $X1,$ but feel free to go in and change this as we continue in the cells below. This is the only spot you need to change the data set we are working with while investigating Kernels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e8cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select which data set we want to work with by uncommenting lines.\n",
    "#       Make sure the data set you are not interested is commented, so only\n",
    "#       have 1 uncommented at a time\n",
    "\n",
    "# To use data set 1, uncomment these lines\n",
    "# X=X1[:,0:2]\n",
    "# Y=X1[:,2]\n",
    "\n",
    "# To use data set 2, uncomment these lines\n",
    "X=X2[:,0:2]\n",
    "Y=X2[:,2]\n",
    "\n",
    "# To use data set 3, uncomment these lines\n",
    "# X=X3\n",
    "# Y=Y3\n",
    "\n",
    "# This line formats the class labels so that they will work with out SVMs\n",
    "# Do NOT comment this line out\n",
    "Y=Y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c439c",
   "metadata": {},
   "source": [
    "## Sklearn SVMs\n",
    "\n",
    "Sklearn has a great library for SVMs. There are a number of different implementations of SVMs by Sklearn, and in this session we will be using Sklearn.svm.SVC. This is a good implementation because it is very flexible with the use of kernels and multiclass classification problems. It scales well for problems with up to 10,000 data points. SVC sets the default kernel to 'rbf' but we will begin by using a 'linear' kernel. More information on Sklearn.svm.SVC can be found at https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "Additional information on classifiers provided in the Sklearn library can be found at https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm and https://scikit-learn.org/stable/modules/svm.html#svm. Notibly, the LinearSVC is very similar to the SVC model we are working with, but it is more flexible in terms of which cost function you use and scales well to larger problems. However, it is limited by the fact that it must have a linear kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a536a8",
   "metadata": {},
   "source": [
    "## Initial SVM with Linear Kernel\n",
    "\n",
    "Here we are loading all of our data into an SVM to see what decision boundary will be created. Usually, when creating an model it is important to partition the data in order to have some method for model validation, but in this case since we are using 2 dimensional data we will be able to visually inspect the decision boundary. Later on, when we use SVMs to make predictions, we will partition the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e63b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVC model\n",
    "model = svm.SVC(kernel='linear')\n",
    "# Train the model on the data set you selected above\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08786fe3",
   "metadata": {},
   "source": [
    "The next cell of code shows the decision boundary formed by the model we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f985293f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the decision region created by the SVC model. We will use this function a lot today\n",
    "plot_decision_regions(X, Y, clf=model, legend=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cabe0be",
   "metadata": {},
   "source": [
    "From the plot of the decision boundary above, we see that this model could greatly be improved if it had a nonlinear decision boundary. To do this, we will explore a few different kernels to see how they change the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33352d1",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "Support Vector Machines (SVMs) are powerful tools for machine learning, and they become even more powerful when a kernel is used to define a more complex decision boundary. The Sklearn library provides a number of kernels that can be used in the SVC model we are using. We will have already seen a linear kernel, so now we will repeate the steps above using the following kernels\n",
    "1. rbf (default kernel for SVC)\n",
    "2. poly\n",
    "3. sigmoid\n",
    "\n",
    "Each type of kernel has different parameterizations based on its mathematical formulation. Additional information on the kernels used in sklearn can be found at https://scikit-learn.org/stable/modules/svm.html#svm-kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f936486",
   "metadata": {},
   "source": [
    "### Rbf Kernel\n",
    "The rbf kernel uses the exponential function\n",
    "<center> $exp(-\\gamma\\|X-X'\\|)$\n",
    "    \n",
    "to measure the distance between the points $X$ and $X'.$ When viewing the decision boundary from the model below, vary the parameter gamma to see how it changes the decision boundary (gamma > 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVM with a rbg kernel\n",
    "# TODO: Modify the gamma value of the kernel\n",
    "model = svm.SVC(kernel='rbf', gamma = )\n",
    "# Fit the model to the data set selected above\n",
    "model.fit(X,Y)\n",
    "# Show the decision boundary we created\n",
    "plot_decision_regions(X, Y, clf=model, legend=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b1b71",
   "metadata": {},
   "source": [
    "### Polynomial Kernel\n",
    "The polynomial kernel uses the function \n",
    "<center>$(\\gamma\\langle X,X'\\rangle+r)^d$\n",
    "    \n",
    "to measure the distance between the points $X$ and $X'.$ By default, $d$ will be set to 3, but this can be modified in the parameters of the model. Again, you can vary the parameters to see how it affects the decision boundary ($d$ is given as degree and $r$ is given as coef0).\n",
    "    \n",
    "Note: the run time scales with $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f66eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVM with a polynomial kernel and modify the parameters\n",
    "\n",
    "# TODO: Modify the gamma, degree and coef0 values of the kernel.\n",
    "#       If you make degree too large, it will take longer\n",
    "#       I recommend degree <= 6\n",
    "model = svm.SVC(kernel='poly', gamma = , degree = , coef0 = )\n",
    "# Train the model on the data set you selected above\n",
    "model.fit(X,Y)\n",
    "# Show the decision boundary that the SVC created\n",
    "plot_decision_regions(X, Y, clf=model, legend=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51e739",
   "metadata": {},
   "source": [
    "### Sigmoid Kernel\n",
    "\n",
    "The sigmoid kernel uses the function\n",
    "<center>$\\tanh(\\gamma\\langle X,X'\\rangle+r)$\n",
    "    \n",
    "to measure the distance between the points $X$ and $X'$\n",
    "    \n",
    "The $tanh(x)$ function we see here has the following graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf0539c",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "\n",
    "<body>\n",
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "    <img src=\"imgs/tanh.png\" alt=\"tanh function\" style=\"width: 400px\">\n",
    "    <center>Iris Setosa</center>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee034a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SVM with a signoid kernel and modify the parameters\n",
    "\n",
    "# TODO: Modify the gamma and coef0 parameters of the kernel\n",
    "# NOTE: Sometimes this kernel doesn't work well with X3\n",
    "model = svm.SVC(kernel='sigmoid', gamma = 2, coef0 = 5)\n",
    "# Train the SVC on the data set you selected above\n",
    "model.fit(X,Y)\n",
    "# Plot the decision boundary\n",
    "plot_decision_regions(X, Y, clf=model, legend=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619952f7",
   "metadata": {},
   "source": [
    "The point of the above code is to give some intuition for how kernels impact a SVM and an understanding how why distance is such an important concept in machine learning. It is important to note that in the above code, we have only use 1 type of machine learning model, but we see that by varying the kernel we apply to the model we get varying results.\n",
    "\n",
    "Typically, a kernel should NOT be selected by trying to visualize the decision boundary. This is impractical for datasets in higher dimensions, and it would discount the importance of having a validation method. Kernel selection should go through a process similar to the model validation process we have discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445adb89",
   "metadata": {},
   "source": [
    "## Multiclass Classification with SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b2f9f",
   "metadata": {},
   "source": [
    "The 3 main types of machine learning problems are\n",
    "1. Supervised learning (Classification)\n",
    "2. Unsupervised learning (Clustering)\n",
    "3. Reinforcement learning\n",
    "\n",
    "So far we have seen a number of supervised learning or classification algorithms (neural networks, Naive Bayes, SVMs, etc.). Generally, these classification algorithms, and especially SVMs are used for binary classification problems. However, in many cases there are more than 2 classes in a supervised learning problem. In this section, we will cover the 2 main methods, one vs. rest and one vs. one, which are adaptations of binary classifiers for multiclass problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21588521",
   "metadata": {},
   "source": [
    "### One vs. One (OvO) Classification\n",
    "\n",
    "Given a classification problem involving $K$ classes, $K(K-1)/2$ binary classifiers are created. Each of these binary classifiers is similar to the SVMs we have previously seen, and at training time, each classifier is passed all of the training examples from 2 classes and learns to distinguish between them (For a $K$ class problem, there are $K(K-1)/2$ pairs of classes). At testing time, given a new data point, all $K(K-1)/2$ classifiers are run on that point and vote to which class is should belong to. After the voting is complete, the model labels the test data as the class that had the highest number of votes. OvO classification is how the SVC model we have been using handles multiclass classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ed5e9",
   "metadata": {},
   "source": [
    "### One vs. Rest (OvR) Classification\n",
    "\n",
    "One vs. Rest classification, also called One vs. All (OvA) or One Against All (OAA), is a method for a $K$ class classification problem where $K$ binary classifiers are trained to determine if a data point belongs to a class or not. At training time, each binary classifier is passed all of the training data where the samples of one class are positive and all other samples are negative (Each binary classifier will have a different class be positive). This method of partitinoing the training data causes each binary classifier to be trained on unbalanced data. Then at test time, the test data is passed to all binary classifiers and, similar to OvO classification, the binary classifiers vote for which class the test data belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592136e",
   "metadata": {},
   "source": [
    "### Voting Schemes\n",
    "\n",
    "Both OvO and OvR classification schemes rely on binary classifiers to vote, and a potential issue that can arries here is ties in voting. In the case of OvO, if there is a tie, the model needs a method for breaking the tie. In OvR, since there is only 1 model that can vote for each class, if there is a tie, the tie will have values of either 1-1 or 0-0 (it could be a multiclass tie). To avoid this, each in OvR each binary classifier can give a confidence value for its vote, so in the case of a tie the model defaults to the binary classifier with the highest confidence. This method of confidence intervals is not perfect, and unbalanced training data can lead to variation in the scale of the confidence values.\n",
    "\n",
    "\n",
    "Make description shorter "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05abd511",
   "metadata": {},
   "source": [
    "## Iris Data Set\n",
    "\n",
    "The Iris Data Set is a famous a popular data set for machine learning. The data set contains measurements from 150 flowers belonging to 3 different species of Iris flowers (50 flowers per species). This data set was introduced in 1936 by Robert Fisher and remains a popular data set for practicing multiclass classification problems. For each flower, the dataset contains the features:\n",
    "   1. sepal length in cm\n",
    "   2. sepal width in cm\n",
    "   3. petal length in cm\n",
    "   4. petal width in cm\n",
    "   \n",
    "While this data set is 4 dimensional, it is known that 1 of the classes is linearly separable from the other 2, but the other 2 are not linearly separable from each other. This makes it interesting for applying our new knowledge of Kernels. The goal is to use these features to classify a flower as belonging to 1 of these 3 species:\n",
    "1. Iris Setosa\n",
    "2. Iris Versicolour\n",
    "3. Iris Virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df90ff",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "\n",
    "<body>\n",
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "    <img src=\"imgs/iris_setosa.jpg\" alt=\"Snow\" style=\"width: 200px\">\n",
    "    <center>Iris Setosa</center>\n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "    <img src=\"imgs/iris_versicolour.jpg\" alt=\"Forest\" style=\"width: 200px\">\n",
    "    <center>Iris Versicolour</center>\n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "    <img src=\"imgs/iris_virginica.jpg\" alt=\"Mountains\" style=\"width: 200px\">\n",
    "    <center>Iris Virginica</center>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca883766",
   "metadata": {},
   "source": [
    "### Load Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496cd763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "df = pd.read_csv('data/iris.data')\n",
    "# Extract the feature vectors\n",
    "X_df = df.iloc[:,0:4]\n",
    "X = X_df.to_numpy()\n",
    "#Y_df = df.iloc[:,4:5]\n",
    "# Make the data labels. They can be made this way because we know the order of the data set\n",
    "Y = np.zeros((150,))\n",
    "counter = 0\n",
    "while counter < 150:\n",
    "    if counter < 50:\n",
    "        Y[counter] = 1\n",
    "    elif counter < 100:\n",
    "        Y[counter] = 2\n",
    "    else:\n",
    "        Y[counter] = 3\n",
    "    counter = counter + 1\n",
    "Y = Y.astype(int)\n",
    "# Split the data set into training and testing sets\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(X, Y, test_size=0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d33714",
   "metadata": {},
   "source": [
    "### Explore the Data Set\n",
    "Since this data is 4 dimensional, there is no simple way to visualize it in the plane. However, we are able to project the 4D data into 2D to visualize it. By changing the indecies in the scatter plot below, we can visualize the data projected into different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc666206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the plot size\n",
    "plt.rcParams['figure.figsize'] = [12, 10]\n",
    "# Set it to make 6 plots\n",
    "fig, axs = plt.subplots(3,2)\n",
    "# Plot projection of the data set\n",
    "axs[0,0].scatter(data_train[:, 0], data_train[:, 1], c=labels_train)\n",
    "axs[0,0].set_xlabel('Sepal Length')\n",
    "axs[0,0].set_ylabel('Sepal Width')\n",
    "axs[0,1].scatter(data_train[:, 0], data_train[:, 2], c=labels_train)\n",
    "axs[0,1].set_xlabel('Sepal Length')\n",
    "axs[0,1].set_ylabel('Petal Length')\n",
    "axs[1,0].scatter(data_train[:, 0], data_train[:, 3], c=labels_train)\n",
    "axs[1,0].set_xlabel('Sepal Length')\n",
    "axs[1,0].set_ylabel('Petal Width')\n",
    "axs[1,1].scatter(data_train[:, 1], data_train[:, 2], c=labels_train)\n",
    "axs[1,1].set_xlabel('Sepal Width')\n",
    "axs[1,1].set_ylabel('Petal Width')\n",
    "axs[2,0].scatter(data_train[:, 1], data_train[:, 3], c=labels_train)\n",
    "axs[2,0].set_xlabel('Sepal Width')\n",
    "axs[2,0].set_ylabel('Petal Width')\n",
    "axs[2,1].scatter(data_train[:, 2], data_train[:, 3], c=labels_train)\n",
    "axs[2,1].set_xlabel('Petal Length')\n",
    "axs[2,1].set_ylabel('Petal Width')\n",
    "# Reset the default size of the plots\n",
    "plt.rcParams['figure.figsize'] = [6.4, 4.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14dc98",
   "metadata": {},
   "source": [
    "### Explore Different SVM Models for Multiclass Classification with Kernels\n",
    "One method to explore how to develope the best classifier is to find a classifier that performes well at seperating these 3 classes in each of the 6 above projections. In the cell below, we have defined a function to test out a classifier in each of projection and plot th decision boudaries, similar to what we did when exploring the different types of kernel functions and their parameters. While this is not a foolproof method for model selection, generally if a single model preforms well in all projections of the feature space, then it will preform well when the entire feature space is used, and if a model doesn't preform well in any of the projections, then it is likely not going to work well when the entire feature space is used.\n",
    "\n",
    "Note: This type of experiment will only work because the feature space is 4D. As we saw above, there are only 6 projections from 4D to 2D, so we are able to view them all. However, as the size of the feature space grows, the number of projections will grow very quickly. (In an $n-$dimensional feature space, the number of projections is equal to the number of pairs of features, which is given by $n$ choose 2 or $\\binom{n}{2}=\\frac{n!}{2!(n-2)!}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_projections(model, X, y):\n",
    "    # Set the plotting parameters so we can see all 6 projections\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(12, 10))\n",
    "    # Count which number chart we have made\n",
    "    chart = 0\n",
    "    # Outter is used to iterate over 1 of the features we will be using for the projection\n",
    "    outter = 0\n",
    "    while outter < 3:\n",
    "        # Inner is used to iterate over the 2nd feature we will be using for the projection\n",
    "        inner = outter + 1\n",
    "        while inner < 4:\n",
    "            # Project the feature space to only the features defined by the outter and inner counters\n",
    "            X_temp = np.array([X[:,outter], X[:,inner]]).T\n",
    "            # Fit the model to this data projection\n",
    "            model.fit(X_temp, y)            \n",
    "            # Plot the decision boundaries of the model for this given projection\n",
    "            fig = plot_decision_regions(X_temp, y, clf=model, ax=axs[int(np.floor(chart/2))][int(chart%2)], legend=2)\n",
    "            # print('[', str(int(np.floor(chart/2))), ' : ', str(int(chart%2)), ']')\n",
    "            # Increment the chart counter and the counters used to select which features are used in the projection\n",
    "            chart = chart + 1\n",
    "            inner = inner + 1\n",
    "        outter = outter + 1\n",
    "    # axs[0,0].scatter(data_train[:, 0], data_train[:, 1], c=labels_train)\n",
    "    axs[0,0].set_xlabel('Sepal Length')\n",
    "    axs[0,0].set_ylabel('Sepal Width')\n",
    "    # axs[0,1].scatter(data_train[:, 0], data_train[:, 2], c=labels_train)\n",
    "    axs[0,1].set_xlabel('Sepal Length')\n",
    "    axs[0,1].set_ylabel('Petal Length')\n",
    "    # axs[1,0].scatter(data_train[:, 0], data_train[:, 3], c=labels_train)\n",
    "    axs[1,0].set_xlabel('Sepal Length')\n",
    "    axs[1,0].set_ylabel('Petal Width')\n",
    "    # axs[1,1].scatter(data_train[:, 1], data_train[:, 2], c=labels_train)\n",
    "    axs[1,1].set_xlabel('Sepal Width')\n",
    "    axs[1,1].set_ylabel('Petal Width')\n",
    "    # axs[2,0].scatter(data_train[:, 1], data_train[:, 3], c=labels_train)\n",
    "    axs[2,0].set_xlabel('Sepal Width')\n",
    "    axs[2,0].set_ylabel('Petal Width')\n",
    "    # axs[2,1].scatter(data_train[:, 2], data_train[:, 3], c=labels_train)\n",
    "    axs[2,1].set_xlabel('Petal Length')\n",
    "    axs[2,1].set_ylabel('Petal Width')\n",
    "    # Reset the default size of the plots\n",
    "    plt.rcParams['figure.figsize'] = [6.4, 4.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8666f",
   "metadata": {},
   "source": [
    "Now that we have defined the above function, in the next cell, we can pass a model and the Iris data into the function to visualize how well our model works in each projection. You should try a variety of kernel functions and parameters to see which are effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed4ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVM with a linear kernel\n",
    "model = svm.SVC(kernel='linear')\n",
    "# Plot the decision boundaries for this model and the training data\n",
    "plot_projections(model, data_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bdd24c",
   "metadata": {},
   "source": [
    "### Model Validation\n",
    "\n",
    "Now that we have learned a little about SVMs, kernels, and multiclass classification, it is time to preform model validation and testing. So far, we have only been exposed to the training data, but our goal is to develope a model that will generalize well to the remaining data from the Iris data set. To do this, we will preform cross validation on a model that preformed well in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, y, k, model):\n",
    "    # StratifiedKFold is a function to split the data into training and validation sets\n",
    "    skf = StratifiedKFold(n_splits=k) # k is a parameter passed to the function\n",
    "    # This loop executes 1 time per fold, k times total\n",
    "    for train_indices, test_indices in skf.split(X,y):\n",
    "        # Get the training and validation data\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        Y_train, Y_test = y[train_indices], y[test_indices]\n",
    "        # Train the model on the training data\n",
    "        model.fit(X_train, Y_train)\n",
    "        # Get the predictions from the validation data\n",
    "        predictions=model.predict(X_test)\n",
    "        print(predictions)\n",
    "        # Create the confusion matrix based on the predictions and the true labels of the validation data\n",
    "        confusion_mat=metrics.confusion_matrix(Y_test, predictions, labels = [1, 2, 3])\n",
    "        # Show the confusion matrix\n",
    "        print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8175c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVM.\n",
    "# TODO: Choose the kernel and its parameters\n",
    "model = svm.SVC(kernel = , )\n",
    "# Call the function above with the model we created in the previous line\n",
    "k_fold_cross_validation(data_train, labels_train, 6, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d1e1f7",
   "metadata": {},
   "source": [
    "## Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e7522",
   "metadata": {},
   "source": [
    "Now that we have explored some of the basics of SVMs and their Kernel function, we will apply this knowledge to predict which cells are cancerous. The data set below contains information on the following 10 features of cell nuclie from cells found in breast masses. These features include:\n",
    "1. radius (mean of distances from center to points on the perimeter)\n",
    "2. texture (standard deviation of gray-scale values)\n",
    "3. perimeter\n",
    "4. area\n",
    "5. smoothness (local variation in radius lengths)\n",
    "6. compactness (perimeter^2 / area - 1.0)\n",
    "7. concavity (severity of concave portions of the contour)\n",
    "8. concave points (number of concave portions of the contour)\n",
    "9. symmetry\n",
    "10. fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "In each image and for each of these features, the mean, standard error, and mean of the 3 largest values was recored, resulting in the 30D dataset we will explore below.\n",
    "\n",
    "This data set is not evenly weighted. Of the almost 600 examples we have, almost 2/3 of them are malignant and only 1/3 is benign. \n",
    "\n",
    "When loading this data set, we will all have the exact same training set, which should be used for training and validating our models, and we will all have the same test data sets. Using our knowledge of SVMs, the goal is to acheive the highest performance metrics on the test set.\n",
    "\n",
    "This data set was originally generated at the University of Wisconsin by Dr. William H. Wolberg, W. Nick Street, and Olvi L. Mangasarian. More information on the data set can be found at https://www.kaggle.com/uciml/breast-cancer-wisconsin-data?select=data.csv and https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d37458c",
   "metadata": {},
   "source": [
    "### Load the data set and partition it for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8739080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data set\n",
    "df = pd.read_csv('data/cancer.csv')\n",
    "# Select which columns we want to keep. By default this includes everything\n",
    "df = df[['diagnosis','radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean','compactness_mean','concavity_mean','concave points_mean','symmetry_mean','fractal_dimension_mean','radius_se','texture_se','perimeter_se','area_se','smoothness_se','compactness_se','concavity_se','concave points_se','symmetry_se','fractal_dimension_se','radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst','compactness_worst','concavity_worst','concave points_worst','symmetry_worst','fractal_dimension_worst']]\n",
    "# Do NOT modify this line of code\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40fbb61",
   "metadata": {},
   "source": [
    "While there are 30 features to select from in the data set, we could train our model on fewer features. In the next cell, we define the 'used_features' array that contains all of the features we want to include. By default, this array contains all of the features in the data set, but feel free to try removing some.\n",
    "\n",
    "### Return here after calculating information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify the cell during the first time. After we examine information gain of each feature, we will return here\n",
    "\n",
    "# TODO: Select which fields in the data set you want to include\n",
    "used_features = ['radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean','compactness_mean','concavity_mean','concave points_mean','symmetry_mean','fractal_dimension_mean','radius_se','texture_se','perimeter_se','area_se','smoothness_se','compactness_se','concavity_se','concave points_se','symmetry_se','fractal_dimension_se','radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst','compactness_worst','concavity_worst','concave points_worst','symmetry_worst','fractal_dimension_worst']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2dab3",
   "metadata": {},
   "source": [
    "Based on the 'used_features' array in the cell above, this next cell works to split up the training and testing features and labels into numpy arrays, only including the features we selected above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e517bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing data and labels\n",
    "training_labels_temp = train[['diagnosis']]\n",
    "training_features = train[used_features]\n",
    "testing_labels_temp = test['diagnosis']\n",
    "testing_features = test[used_features]\n",
    "\n",
    "# Extract the values in the feature vectors\n",
    "training_features = training_features.values\n",
    "testing_features = testing_features.values\n",
    "\n",
    "# Give the training label vector a numeric format\n",
    "labels = training_labels_temp.values\n",
    "training_labels = np.zeros((len(labels),))\n",
    "for i in range(len(training_labels)):\n",
    "    if str(labels.T[0][i]) == 'B':\n",
    "        training_labels[i] = 1\n",
    "        \n",
    "# Give the testing label vector a numeric format\n",
    "labels = testing_labels_temp.values\n",
    "testing_labels = np.zeros((len(labels),))\n",
    "for i in range(len(testing_labels)):\n",
    "    if str(labels.T[i]) == 'B':\n",
    "        testing_labels[i] = 1\n",
    "\n",
    "# Make the label vectors of type int so that they work with our SVC models\n",
    "training_labels = training_labels.astype(int)\n",
    "testing_labels = testing_labels.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20855925",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Calculate Information Gain\n",
    "Now that we have loaded and partitioned our data, we will calculate the information gain for each feature. Sklearn has the function \"mutual_info_classif\" that can be used to calculate information gain, similar to what we have seen in lecture. More information on the implementation of this can be found at: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c27d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the information gain for each feature in the data set\n",
    "information = mutual_info_classif(training_features, training_labels)\n",
    "# Place the features and information for each feature in a dictionary\n",
    "information_gain = dict(zip(information, used_features))\n",
    "# Print out the dictionary we just created, with the most significant features first\n",
    "for i in sorted(information_gain, reverse = True):\n",
    "    print(str(i) + \": \" + str(information_gain[i]))\n",
    "# Print the number of features in the dictionary, so that we can verify that there are\n",
    "# as many features in the dictionary as we selected when we created the 'used_features' array\n",
    "print(len(information_gain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad29bd11",
   "metadata": {},
   "source": [
    "Now that we have calculated information gain for each feature, we can go back to the previous cell where we define the \"used_feature\" array and modify it to only include the features with the highest information gain. Once we remove the unnecessairy features from the \"used_feature\" vector, we should rerun the subsequent cells to reload the data with only the selected features and again partition it into training and testing sets. After reloading and partitioning the data is complete, you can again calculate information gain using the cell above and repeat this process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf808ad",
   "metadata": {},
   "source": [
    "### Calculating Performance Metrics from the Confusion Matrix\n",
    "\n",
    "In the next cell, we define a function that we will use to calculate some basic performance metrics from a confusion matrix. These metrics are defined by the same equations seen in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(confusion_matrix, metric):\n",
    "    # TODO: Fill in the indecies of the confusion matrix\n",
    "    TP = confusion_matrix[0][0] # True Positive\n",
    "    FP = confusion_matrix[0][1] # False Positive\n",
    "    FN = confusion_matrix[1][0] # False Negative\n",
    "    TN = confusion_matrix[1][1] # True Negative\n",
    "    total = TP + FP + FN + TN\n",
    "    if metric == 'Accuracy':\n",
    "        return (TP + TN) / total\n",
    "    elif metric == 'Sensitivity':\n",
    "        return TP / (TP + FN)\n",
    "    elif metric == 'Specificity':\n",
    "        return TN / (TN + FP)\n",
    "    elif metric == 'False Positive Rate':\n",
    "        return FP / (FP + TN)\n",
    "    elif metric == 'Precision':\n",
    "        return TP / (TP + FP)\n",
    "    elif metric == 'F1 Score':\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN) # Recall is sometimes called sensitivity\n",
    "        return 2 * recall * precision / (recall + precision)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d506a",
   "metadata": {},
   "source": [
    "In the next cell, select which metric you want to focus on in the following sections. Rather than looking at every metric at once, we focus on one at a time. Feel free to change this as you explore the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d302ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select which performance metric you want to work with\n",
    "my_metric = 'Accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2378395e",
   "metadata": {},
   "source": [
    "### Training Our Model\n",
    "\n",
    "Now that this performance metric function is defined, we can create a function for cross validation and use it to calculate the preformance of different models on our training data. In the next cell, we define a function for cross validation. This function is slightly different than the function we used with the Iris data set because it calls \"performance_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_fold_cross_validation(X, y, k, model, metric):\n",
    "    # These arrays will be used to store the performance metrics from each fold\n",
    "    metric_by_fold = np.zeros((k,1))\n",
    "    fold = 0\n",
    "    \n",
    "    # This is a function that we will use to split the data into training and validation sets\n",
    "    skf = StratifiedKFold(n_splits=k)\n",
    "    for train_indices, test_indices in skf.split(X,y):\n",
    "        \n",
    "        # Get the training and validation data\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        Y_train, Y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "        # Train the model on the training data\n",
    "        model.fit(X_train, Y_train)\n",
    "        # Get the predictions from the validation data \n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Create the confusion matrix based on the predictions and the true labels of the validation data\n",
    "        confusion_matrix = metrics.confusion_matrix(Y_test, predictions, labels = [1, 0])\n",
    "        # Show the confusion matrix\n",
    "        fold_metric = performance_metrics(confusion_matrix, metric)\n",
    "        \n",
    "        # Make sure we have a valid performance metric\n",
    "        if fold_metric == None:\n",
    "            print('N fold cross validation exited because you entered an invalid metric.')\n",
    "            return\n",
    "        \n",
    "        # TODO: Copy the metric into the array for each fold\n",
    "        metric_by_fold[fold] = \n",
    "        # Increment the fold counter\n",
    "        fold += 1\n",
    "    \n",
    "    # Calculate the mean and standard deviation for your performance metric\n",
    "    print(metric, ' Mean: ', str(np.mean(metric_by_fold)))\n",
    "    print(metric, ' Standard Deviation: ', str(np.std(metric_by_fold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0485f6c",
   "metadata": {},
   "source": [
    "We are now ready to create our model. In the following cell, define your classifier. Select a kernel and parameters for it. An additional parameter that can be set is \"class_weight.\" You may have noticed that almost 2/3 of the examples in the training set are malignant (which is given a value of 1 in our code). \"class_weight\" lets us create the model so that the high frequency of occurence of malignant cells does not prevent the classifier from being effective. A numeric value for each class in the classifier can be given in the form of a dictionary (as seen in the starter code) or there are other ways this parameter can be set automatically. More information can on \"class_weight\" can be found under the parameter section on this page: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVC classifier with whatever parameters you choose\n",
    "\n",
    "# TODO: Select a kernel and parameters for it\n",
    "#       class_weight is another parameter we can modify\n",
    "model = svm.SVC(kernel = , class_weight={1: 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812781f",
   "metadata": {},
   "source": [
    "Now that the model has been created, it can be passed to the cross validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a340b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set n, the number of folds for cross validation\n",
    "n = \n",
    "# Pass the model we created and the training data to the cross validation function\n",
    "n_fold_cross_validation(training_features, training_labels, n, model, my_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b21b50",
   "metadata": {},
   "source": [
    "You should try multiple models and parameter setting in the cells above to see which are likely to maximize whichever performance metrics you are interested. This should take multiple iterations since determining the optimal classifier is not a simple task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f84c301",
   "metadata": {},
   "source": [
    "### Test Your Model\n",
    "\n",
    "So far, we have only worked with the training portion of this data set. As with all machine learning problems, it is important to have a clear separation between training and test data, which is why our parameter tuning with the cross validation that we just preformed only used the training data. When good data is collected, the training data is representative of the test data, so if a model preformed well in training it should have similarly good preformance metrics on the testing data.\n",
    "\n",
    "Once you are satisfied with your results from cross validation and they are consistent, you can execute the following code to see how well it classifies the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model on the training data\n",
    "model.fit(training_features, training_labels)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = model.predict(testing_features)\n",
    "\n",
    "# Generate a confusion matrix based on the testing data predictions and true labels\n",
    "confusion_matrix = metrics.confusion_matrix(testing_labels, predictions, labels = [1, 0])\n",
    "\n",
    "# Calculate the preformance metrics from the confusion matrix\n",
    "metric_value = performance_metrics(confusion_matrix, my_metric)\n",
    "print('Test', my_metric, ':', metric_value)\n",
    "\n",
    "# TODO: Discuss your results with everyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc6a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
