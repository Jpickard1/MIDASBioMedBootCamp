{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv \n",
    "from pandas import DataFrame\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM  \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Activity Recognition Using Smartphones Data Set\n",
    "\n",
    "We will be using machine learning to preform Human Activity Recognition (HAR) for the following 6 activities:\n",
    "\n",
    "1. WALKING\n",
    "2. WALKING_UPSTAIRS\n",
    "3. WALKING_DOWNSTAIRS\n",
    "4. SITTING\n",
    "5. STANDING\n",
    "6. LAYING\n",
    "\n",
    "To do this, we will analyze 9 signals collected from people as they preformed these activities. 30 participants wore a Samsung Galaxy S II. Using its embedded accelerometer and gyroscope, 3-axial linear acceleration and 3-axial angular velocity were recorded at a constant rate of 50Hz.\n",
    "\n",
    "The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. Note: it is important that all of a patients activities were recorded in either the training or testing sets and not split between the 2.\n",
    "\n",
    "The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain. See 'features_info.txt' for more details.\n",
    "\n",
    "For more information, please consult the README file in the data folder or visit: https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "To load this function, we import `utils`. This is a python file saved in the same directory as the file we are working in, and it has some helper functions for loading the data. We need these helper files because as we will see this data is more complicated than some of the other classification problems we have discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the model to load the training and testing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainy, testX, testy = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we print the shape of each part of the partitioned data set we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print the shape of each part of the data set (i.e. featues and labels for testing and training)\n",
    "print('Training Features Shape:',   )\n",
    "print('Training Labels Shape:',   )\n",
    "print('Testing Features Shape:',   )\n",
    "print('Testing Labels Shape:',   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate the Data\n",
    "\n",
    "### Class Balance\n",
    "As we have seen, it is important to understand the class balance of a data set. In the case of a HAR classification problem, we are interested in knowing how many examples of each type of activity we have in the data set. In the following cell, we explore this. Additionally, this is the first multiclass classification problem that we will be working on that doesn't have perfectly balanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the numpy array into a dataframe\n",
    "df = DataFrame(trainy) \n",
    "# group data by the class value and calculate the number of rows\n",
    "counts = df.groupby(0).size()\n",
    "# retrieve raw rows\n",
    "counts = counts.values\n",
    "# summarize\n",
    "print('Training Data Set:')\n",
    "for i in range(len(counts)):\n",
    "    percent = counts[i] / len(df) * 100\n",
    "    print('Class=%d, total=%d, percentage=%.3f' % (i+1, counts[i], percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting a Single Activity\n",
    "\n",
    "Since we have time serise data, an important check is plotting all of the data in time. We can do this for a signle subject at a time to get a sense of what the signals may look like. The function below will make a plot for the value of each signal throughout time as well as a plot for which activitiy is occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data for one subject\n",
    "def plot_subject(X, y):\n",
    "    plt.figure()\n",
    "    # determine the total number of plots\n",
    "    n, off = X.shape[2] + 1, 0\n",
    "    # plot total acc\n",
    "    for i in range(3):\n",
    "        plt.subplot(n, 1, off+1)\n",
    "        plt.plot(to_series(X[:, :, off]))\n",
    "        plt.title('total acc '+str(i), y=0, loc='left')\n",
    "        off += 1\n",
    "    # plot body acc\n",
    "    for i in range(3):\n",
    "        plt.subplot(n, 1, off+1)\n",
    "        plt.plot(to_series(X[:, :, off]))\n",
    "        plt.title('body acc '+str(i), y=0, loc='left')\n",
    "        off += 1\n",
    "    # plot body gyro\n",
    "    for i in range(3):\n",
    "        plt.subplot(n, 1, off+1)\n",
    "        plt.plot(to_series(X[:, :, off]))\n",
    "        plt.title('body gyro '+str(i), y=0, loc='left')\n",
    "        off += 1\n",
    "    # plot activities\n",
    "    plt.subplot(n, 1, n)\n",
    "    plt.plot(y)\n",
    "    plt.title('activity', y=0, loc='left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `to_series` function we see above is used to account for the fact that there is a 50% overlap in the signals between time points. This function, is used to remove that overlap. If you want to explore how that function works, you can look in `utils.py` to see it, and some of the other functions we have been relying on for loading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The subjects are the patients we will be loading in from the training set\n",
    "sub_map = load_file('data/train/subject_train.txt')\n",
    "\n",
    "# This loads in the list of unique subjects\n",
    "train_subjects = np.unique(sub_map)\n",
    "print(train_subjects)\n",
    "\n",
    "# TODO: Fill in this line to print the number of subjects\n",
    "print('There are',    ,'subjects')\n",
    "\n",
    "# get the data for one subject\n",
    "# TODO: This value must be with in the range of train_subjects\n",
    "sub_id = train_subjects[     ]\n",
    "\n",
    "# Load the features and labels for the subject\n",
    "subX, suby = data_for_subject(trainX, trainy, sub_map, sub_id)\n",
    "print(subX.shape, suby.shape)\n",
    "\n",
    "# plot data for subject\n",
    "plt.rcParams['figure.figsize'] = [16, 16]\n",
    "plot_subject(subX, suby)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Developement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we reformat the labels to be compatable with the `Sequential` model from `tensorflow`. This reformats the labels into One Hot Encodings, which is a different format than we used when generating the plots above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy = to_categorical(trainy)\n",
    "testy = to_categorical(testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will build our network, parameterizing the size and activation function of each layer in the network. There are many parameters that we could change here, but because of the time and resources it will take to train, we will keep our model relatively simple during the session and only be varying which activation function we use. Information on these activation functions can be found at: https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    \n",
    "    # TODO: Set these parameters\n",
    "    #       Note: please keep epochs < 5 during the session so it doesn't take too long to run your code\n",
    "    epochs = \n",
    "    batch_size = \n",
    "    \n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "\n",
    "    # TODO: Look at how the model is built\n",
    "    model = Sequential()\n",
    "    # LSTM layer\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    # TODO: fill in the activations as sigmoid, softmax, relu, tahn, or anything else from the link above\n",
    "    model.add(Dense(100, activation=     ))\n",
    "    model.add(Dense(n_outputs, activation=     ))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we creat a function called `run_experiment`, its purpose is to go call the `evaluate_model` function multiple times to record and summarize its scores. It is important to note that this is NOT cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an experiment\n",
    "def run_experiment(repeats=10):\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        \n",
    "        # Call the function we wrote abote\n",
    "        # TODO: Fill in the parameters for that function\n",
    "        score = evaluate_model(   ,   ,   ,   )\n",
    "        score = score * 100.0\n",
    "        print('>#', r+1  , ': ', score  )\n",
    "        scores.append(score)\n",
    "    \n",
    "    # summarize results\n",
    "    # TODO: Fill out this code\n",
    "    print('Mean Accuracy:',   )\n",
    "    print('Standard Deviation in Accuracy:',   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the code in `run_experiment` by calling the function in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on python code seen at these following links:\n",
    "1. https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/\n",
    "2. https://machinelearningmastery.com/how-to-model-human-activity-from-smartphone-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
