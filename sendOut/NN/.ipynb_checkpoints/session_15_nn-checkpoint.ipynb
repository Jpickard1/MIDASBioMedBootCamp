{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planar data classification with one hidden layer\n",
    "\n",
    "Welcome to your day4 programming section. It's time to build your first neural network, which will have a hidden layer. You will see a big difference between this model and the one you implemented using logistic regression. \n",
    "\n",
    "**You will learn how to:**\n",
    "- Implement a 2-class classification neural network with a single hidden layer\n",
    "- Use units with a non-linear activation function, such as tanh \n",
    "- Compute the cross entropy loss \n",
    "\n",
    "Reference:\n",
    "- http://scs.ryerson.ca/~aharley/neural-networks/\n",
    "- http://cs231n.github.io/neural-networks-case-study/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - import python files ##\n",
    "\n",
    "Let's first import all the packages that you will need during this assignment.\n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [sklearn](http://scikit-learn.org/stable/) provides simple and efficient tools for data mining and data analysis. \n",
    "- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python.\n",
    "- testCases provides some test examples to assess the correctness of your functions\n",
    "- planar_utils provide various useful functions used in this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "# Dataset\n",
    "import sklearn.datasets\n",
    "# Model\n",
    "import sklearn.linear_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "\n",
    "# Split train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This is how we usually import written py files to use the functions/classes in those files\n",
    "from testCases import *\n",
    "from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "np.random.seed(1) # set a seed so that the results are consistent\n",
    "\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dataset ##\n",
    "\n",
    "First, let's get the dataset you will work on. The following code will load a \"flower\" 2-class dataset into variables `X` and `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X, Y = load_planar_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the dataset using matplotlib. The data looks like a \"flower\" with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the data:\n",
    "# c: carray-like or list of colors or color, optional\n",
    "# camp: str or Colormap, default: rcParams[\"image.cmap\"] (default: 'viridis')\n",
    "# A Colormap instance or registered colormap name. cmap is only used if c is an array of floats.\n",
    "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have:\n",
    "    - a numpy-array (matrix) X that contains your features (x1, x2)\n",
    "    - a numpy-array (vector) Y that contains your labels (red:0, blue:1).\n",
    "\n",
    "Lets first get a better sense of what our data is like. \n",
    "\n",
    "**Exercise**: How many training examples do you have? In addition, what is the `shape` of the variables `X` and `Y`? \n",
    "\n",
    "**Hint**: How do you get the shape of a numpy array? [(help)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "shape_X = \n",
    "shape_Y = \n",
    "m = \n",
    "### END CODE HERE ###\n",
    "\n",
    "print ('The shape of X is: ' + str(shape_X))\n",
    "print ('The shape of Y is: ' + str(shape_Y))\n",
    "print ('The dataset has m = %d examples!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "       \n",
    "<table style=\"width:20%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td>**shape of X**</td>\n",
    "    <td> (2, 400) </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**shape of Y**</td>\n",
    "    <td>(1, 400) </td> \n",
    "  </tr>\n",
    "  \n",
    "    <tr>\n",
    "    <td>**m**</td>\n",
    "    <td> 400 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross validation**\n",
    "\n",
    "Try to split the dataset into training and test set with the function train_test_split with test_size = 0.2 and random_state = 0.\n",
    "\n",
    "Remenber to transpose the matrix before you input it into the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.T\n",
    "Y = Y.T\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Simple Logistic Regression\n",
    "\n",
    "Before building a full neural network, lets first see how logistic regression performs on this problem. You can use sklearn's built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Train the logistic regression classifier\n",
    "clf = sklearn.linear_model.LogisticRegressionCV();\n",
    "# ravel(): expand dimension\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.ravel.html\n",
    "clf.fit(X_train, np.ravel(y_train));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now plot the decision boundary of these models. Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary for logistic regression\n",
    "plot_decision_boundary(lambda x: clf.predict(x), X.T, Y.T) # Dimensional purpose\n",
    "plt.title(\"Logistic Regression\")\n",
    "\n",
    "# Print accuracy\n",
    "LR_predictions = clf.predict(X_test)\n",
    "accuracy_socre = clf.score(X_test, y_test)\n",
    "\n",
    "# To print a number, we need to first convert it to a string by using \n",
    "print ('Accuracy of logistic regression: %d '+ str(accuracy_socre*100)+\n",
    "       '% ' + \"(percentage of correctly labelled datapoints)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>**Accuracy**</td>\n",
    "    <td> 51.24999999999999% </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: The dataset is not linearly separable, so logistic regression doesn't perform well. Hopefully a neural network will do better. Let's try this now!\n",
    "\n",
    "\n",
    "**Extended reading**:\n",
    "Proof that the decision boundary of logistics regression is linear:https://homes.cs.washington.edu/~marcotcr/blog/linear-classifiers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Neural Network model\n",
    "\n",
    "Logistic regression did not work well on the \"flower dataset\". You are going to train a Neural Network with a single hidden layer.\n",
    "\n",
    "**Here is our model**:\n",
    "<img src=\"images/classification_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "\n",
    "**Reminder**: The general methodology to build a Neural Network is to:\n",
    "\n",
    "    1. Define the neural network structure ( # of input units,  # of hidden units, etc) \n",
    "    2. Compile the model\n",
    "    3. Fit the model\n",
    "    4. Evaluate the model\n",
    "\n",
    "Steps 3-4 are like what we did in Tuesday's lab section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - 4.2  - Defining the neural network structure and compile the model ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Available metrics on Keras for regression and classification tasks:**\n",
    "    \n",
    "https://keras.io/api/metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the neural network structure ( # of input units,  # of hidden units, etc).\n",
    "def build_model(input_dim, #int\n",
    "                first_hidden_layer_node_num = 4, #int\n",
    "                activation_function='sigmoid' #str\n",
    "               ):\n",
    "    # define the keras model\n",
    "    # https://keras.io/api/models/sequential/\n",
    "    model = Sequential()\n",
    "    # The model expects rows of data with 2 variables (the input_dim=2 argument)\n",
    "    # The first hidden layer has 4 nodes and uses the tanh activation function\n",
    "    model.add(Dense(first_hidden_layer_node_num, input_dim=input_dim, activation='tanh'))\n",
    "    # The output layer has one node and uses the sigmoid activation function\n",
    "    model.add(Dense(1, activation=activation_function))\n",
    "    # compile the keras model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC', 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To help understand adam greadient decent method, an illustration is as below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Illustration**: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.\n",
    "\n",
    "<img src=\"images/sgd.gif\" style=\"width:400;height:400;\"> <img src=\"images/sgd_bad.gif\" style=\"width:400;height:400;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Fit the model ####\n",
    "Now we fit the model with training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the keras model on the dataset\n",
    "model = build_model(input_dim = 2, first_hidden_layer_node_num = 4, activation_function='sigmoid')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference of keras model.fit():\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "\n",
    "**batch_size**:\n",
    "\n",
    "Integer or None. Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches).\n",
    "\n",
    "**verbose**:\n",
    "\n",
    "0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\n",
    "\n",
    "**epoch**:\n",
    "\n",
    "Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch, epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs, but merely until the epoch of index epochs is reached.\n",
    "\n",
    "### Reference of model history:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback\n",
    "\n",
    "**Note**\n",
    "F1 metrics have been removed from Keras core. You need to calculate them manually. They removed them on 2.0 version. Those metrics are all global metrics, but Keras works in batches. As a result, it might be more misleading than helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "# store the model into a file\n",
    "mode_path = './simple_nn.h5'\n",
    "callbacks = [ModelCheckpoint(filepath=mode_path)]\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Evaluate the model ####\n",
    "\n",
    "It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of $1$ hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details:\n",
    "\n",
    "- %.2f stands for \"print a float with 2 decimal places\"\n",
    "- %% prints a literal %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the nn model\n",
    "_, auc_value, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"%.2f%%\" % (100 * auc_value))\n",
    "print(\"%.2f%%\" % (100 * accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <td>**AUC on test set**</td>\n",
    "    <td> around 90% percent </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**Accuracy on test set**</td>\n",
    "    <td> around 85% percent </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundary\n",
    "plot_decision_regions(X_test, y_test.reshape(-1), clf=model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is really high compared to Logistic Regression. The model has learnt the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression. \n",
    "\n",
    "Now, let's try out several hidden layer sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 - Tuning hidden layer size (optional/ungraded exercise) ###\n",
    "\n",
    "Run the following code. It may take 1-5 minutes. You will observe different behaviors of the model for various hidden layer sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "- The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. \n",
    "- The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to  fits the data well without also incurring noticable overfitting.\n",
    "- You might want to consider regularization, which lets you use very large models (such as n_h = 50) without much overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6746 - auc: 0.5949 - accuracy: 0.6125\n",
      "AUC:  0.5949488282203674 accuracy:  0.612500011920929 Number of hidden layer:  1\n"
     ]
    }
   ],
   "source": [
    "# Set plot size\n",
    "plt.figure(figsize=(16, 32))\n",
    "# https://www.geeksforgeeks.org/enumerate-in-python/\n",
    "hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 100]\n",
    "\n",
    "auc_values = []\n",
    "accuracy_values = []\n",
    "\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    # Set plot location\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plt.title('Hidden Layer of size %d' % n_h)\n",
    "    model_i = build_model(input_dim = 2, first_hidden_layer_node_num = n_h, activation_function='sigmoid')\n",
    "    # by setting verbose = 0,\n",
    "    # we can let the model train silently without showing progress bar\n",
    "    model_i.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)\n",
    "    plot_decision_regions(X, Y.reshape(-1), clf=model_i)\n",
    "    \n",
    "    _, auc_value, accuracy = model_i.evaluate(X_test, y_test)\n",
    "    auc_values.append(auc_value)\n",
    "    accuracy_values.append(accuracy)\n",
    "    print (\"AUC: \", auc_value, \"accuracy: \", accuracy, \"Number of hidden layer: \", n_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (8,6)\n",
    "plt.title(\"auc, accuracy vs hidden layer size\")\n",
    "# Plot dash lines\n",
    "plt.plot(hidden_layer_sizes, accuracy_values, label=\"Accuracy\")\n",
    "plt.plot(hidden_layer_sizes, auc_values, label=\"AUC\")\n",
    "plt.ylabel(\"Accuracy, AUC\")\n",
    "plt.xlabel(\"hidden layer size\")\n",
    "# Specify legend location and size\n",
    "plt.legend(loc=\"lower left\",fontsize = \"large\")\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional Section**\n",
    "\n",
    "Some optional/ungraded questions that you can explore if you wish: \n",
    "- What happens when you change the tanh activation for a sigmoid activation or a ReLU activation?\n",
    "- Play with the learning_rate. What happens?\n",
    "- What if we change the dataset? (See part 5 below!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Performance on other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can rerun the whole notebook (minus the dataset part) for each of the following datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n",
    "\n",
    "datasets = {\"noisy_circles\": noisy_circles,\n",
    "            \"noisy_moons\": noisy_moons,\n",
    "            \"blobs\": blobs,\n",
    "            \"gaussian_quantiles\": gaussian_quantiles}\n",
    "\n",
    "### START CODE HERE ### (choose your dataset)\n",
    "dataset = \"gaussian_quantiles\"\n",
    "### END CODE HERE ###\n",
    "\n",
    "X, Y = datasets[dataset]\n",
    "X, Y = X.T, Y.reshape(1, Y.shape[0])\n",
    "\n",
    "# make blobs binary\n",
    "if dataset == \"blobs\":\n",
    "    Y = Y%2\n",
    "\n",
    "# Visualize the data\n",
    "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on finishing this Programming Assignment!"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
