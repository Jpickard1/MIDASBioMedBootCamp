{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical operation/data storage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Model \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Evaluation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Data Set\n",
    "\n",
    "Columns description:\n",
    "\n",
    "- age: age in years\n",
    "- sex: (1 = male; 0 = female)\n",
    "- cp: chest pain type\n",
    "- trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n",
    "- chol: serum cholestoral in mg/dl\n",
    "- fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "- restecg: resting electrocardiographic results\n",
    "- thalach: maximum heart rate achieved\n",
    "- exang: exercise induced angina (1 = yes; 0 = no)\n",
    "- oldpeak: ST depression induced by exercise relative to rest\n",
    "- slope: the slope of the peak exercise ST segment\n",
    "- ca: number of major vessels (0-3) colored by flourosopy\n",
    "- thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "- target: refers to the presence of heart disease in the patient (1=yes, 0=no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "heart_df = pd.read_csv('./data/heart.csv', index_col=0)\n",
    "# Shows a few rows of the data set\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows some summary statistics on the dataset\n",
    "heart_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows some basic information about the fields of the data set\n",
    "heart_df.info()\n",
    "\n",
    "# Note that all fields are scalars and we have no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure that we are training our random forest models on balanced data. In the following cell, we see how many examples of healthy and diseased hears we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of diseased examples\n",
    "disease = len(heart_df[heart_df['target'] == 1])\n",
    "# Cound the number of not diseased examples\n",
    "no_disease = len(heart_df[heart_df['target']== 0])\n",
    "\n",
    "# Print out the counts of each class\n",
    "# TODO: Print the number of healthy samples we have\n",
    "print(\"Healthy: \", )\n",
    "# TODO: Print the number of diseased samples we have\n",
    "print(\"Heart Diesease: \", )\n",
    "print(\"Total: \", str(no_disease+disease))\n",
    "\n",
    "# Make a Pie chart to represent how many example we have in each class\n",
    "plt.pie([disease, no_disease], labels=['Heart Disease', 'No Disease'], startangle=90)\n",
    "plt.title('Percent for each Example Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to seperate our data into the feature vectors and the labels. We will use the `drop` function to get the feature vectors, and pull a column out of th `pandas` dataframe to get the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature vectors and labels\n",
    "X = heart_df.drop('target',1)\n",
    "y = heart_df['target']\n",
    "\n",
    "# Convert from pandas objects to numpy objects\n",
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split the data into feature vectors and labels, we need to split it into a training data set and a testing data set. This is done using the `train_test_split` function from `sklearn` as we have seen previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Random Forest Classifier\n",
    "\n",
    "Now that the data is formated we are ready to create our first random forest classifier. In `sklearn` the `RandomeForestClassifier` we will be working with takes arguments for the number of estimators in the model (`n_estimator`) and the maximum tree depth for each classifier (`max_depth`). For the first try, fill in these parameters as you see fit, but remember the trade off being made in terms of model complexity and its ability to generalize to new data.\n",
    "\n",
    "For more information on this model and its implementation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Set the parameters of the RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators = , max_depth = )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "Model validation is a key step in machine learning. Below, we have defined a cross validation function that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, y, k, model):\n",
    "    # This will store the accuracy for each fold\n",
    "    accuracy = np.zeros((k,1))\n",
    "    \n",
    "    # This will count which fold we are on\n",
    "    fold = 0\n",
    "    \n",
    "    # This is a function that we will use to split the data into training and validation sets\n",
    "    # TODO: Fill in this line\n",
    "    skf = StratifiedKFold(n_splits = )\n",
    "    \n",
    "    # This loop executes 1 time per fold\n",
    "    for train_indices, test_indices in skf.split(X,y):\n",
    "        \n",
    "        # Get the training and validation data sets\n",
    "        X_train, X_validation = X[train_indices], X[test_indices]\n",
    "        Y_train, Y_validation = y[train_indices], y[test_indices]\n",
    "\n",
    "        # Train the model on the training data\n",
    "        # TODO: Fill in this line\n",
    "        model.fit(   ,   )\n",
    "\n",
    "        # Get the predictions from the validation data\n",
    "        # TODO: Fill in this line\n",
    "        predictions=model.predict(   )\n",
    "                \n",
    "        # Calculate the accuracy\n",
    "        # TODO: Fill in this line\n",
    "        accuracy[fold] = accuracy_score(   ,   )\n",
    "        fold += 1\n",
    "        \n",
    "    return np.mean(accuracy), np.std(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this cross fold validation function, we can test the model we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in k, the number of folds to be used\n",
    "k = \n",
    "\n",
    "# Call the cross fold validation function\n",
    "# TODO: Fill in the parameters for the next line\n",
    "mean_accuracy, std_accuracy = k_fold_cross_validation()\n",
    "\n",
    "# TODO: Fill out the statements to print the results\n",
    "print('Mean Accuracy:', )\n",
    "print('Standard Deviation of Accuracy', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Hyper Parameters\n",
    "\n",
    "Above, you selected the number of classifiers and the max depth of classifier you wanted your model to have, and then you computed accuracy scores using cross validation. Often, it is difficult to have intuition for setting hyper parameters, so it can be useful to do a search for the optimal values. Below, we show one such methof of doing this search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: modify this array to check the number of estimators you are interested in\n",
    "num_estimators = []\n",
    "\n",
    "# TODO: modify this array to check the max_depths values you are interested in\n",
    "max_depth = []\n",
    "\n",
    "# TODO: set the number of folds for cross validation\n",
    "k = \n",
    "\n",
    "# These values will be used to store the best classifier we find\n",
    "max_accuracy_mean = 0\n",
    "max_accuracy_std = 0\n",
    "best_num_estimators = 0\n",
    "best_max_depth = 0\n",
    "\n",
    "# This loop will check all values in the array num_estimators you made above\n",
    "for num_estimator in num_estimators:\n",
    "    # This loop will check all values in the array max_depths you made above\n",
    "    for depth in max_depth:\n",
    "        \n",
    "        # Creates a RandomForestClassifier with the parameters specified by the loops\n",
    "        model = RandomForestClassifier(n_estimators=num_estimator, max_depth=depth)\n",
    "        # Performes cross fold evaluation on this model\n",
    "        # TODO: Fill out the parameters for cross validation\n",
    "        accuracy, std = k_fold_cross_validation(   ,   ,   ,   )\n",
    "        \n",
    "        # Saves the hyperparameters for the best RandomForestClassifier\n",
    "        # TODO: Finish the if statement to save the parameters of this model if it is the best so far\n",
    "        if accuracy >\n",
    "            max_accuracy_mean = accuracy\n",
    "            max_accuracy_std = std\n",
    "            best_num_estimators = num_estimator\n",
    "            best_max_depth = depth\n",
    "            \n",
    "# Prints the best results\n",
    "print('Maximum Accuracy Mean:', max_accuracy_mean)\n",
    "print('Maximum Accuracy Standard Deviation:', max_accuracy_std)\n",
    "print('Number of Estimators:', best_num_estimators)\n",
    "print('Maximum Depth:', best_max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your Random Forest Classifier\n",
    "\n",
    "Now that you have experience making `RandomForestClassifiers` and have spent some time optimizing your hyperparameters, we can test these models on the testing dataset. We all have the same testing data set, so lets see whose model can get the highest accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the parameters based on the best results you found above \n",
    "model = RandomForestClassifier(n_estimators = , max_depth = )\n",
    "\n",
    "# TODO: Fit the model using all of the training data\n",
    "model.fit(   ,   )\n",
    "\n",
    "# TODO: Make predictions on the testing data\n",
    "predictions = model.predict(   )\n",
    "\n",
    "# TODO: Fill in the parameters to calculate training accuracy\n",
    "testing_accuracy = accuracy_score(   ,   )\n",
    "\n",
    "# TODO: Print the results\n",
    "print('Testing Score:',   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case your are interested, some other performance metrics can be calculated for your model below. It is important to remember that the searching process we preformed above for hyperparameter selection was specific to the accuracy score, which we used throughout. If you wanted to optimize for F1 Score, AUC Value, or anything else, a similar process could be done using other scoring metrics. The code in the cell below is just to give you an example of how these metrics can quickly be calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "auc_value = auc(fpr, tpr)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print('F1 Score:', f1)\n",
    "print('AUC Value:', auc_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing features importances\n",
    "\n",
    "Now that we are done developing our model, a question we may ask is which features were most significant in the `RandomForestClassifier`. Fortunatly, `sklearn` gave this model the `feature_importance_` attribute, which is used to rank the significance of each feature based on the gini index. For this excercise, the actual gini score of each feature is not important, but the overall rank order of the features is what we would like to highlight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data=model.feature_importances_, index=heart_df.drop('target',1).columns)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values() \n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "importances_sorted.plot(kind='barh')\n",
    "plt.title('Features Importances')\n",
    "plt.xlabel('Gini Score')\n",
    "plt.savefig('./image/feature_importances.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> reference: https://github.com/goodboychan/goodboychan.github.io/blob/master/_notebooks/2020-06-04-01-Bagging-and-Random-Forests.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
